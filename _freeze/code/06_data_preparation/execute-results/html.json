{
  "hash": "73b0bfef4d2e9cc8bb615a6477fc7b93",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Data Preparation\"\nauthor: \"Shreya D.\"\ndate: '2025-07-10'\noutput: html_document\nlicense: CC-BY-4.0\nemail: shreya.dimri@uni-bielefeld.de\n---\n\nWe have merged the data from different sources after data extraction, cleaned the typos and made it more usable. We have excluded the articles that did not pass full-text screening and the fitness proxies that we could not find a direction for the effect. These are steps done in the script **05_data_cleaning.Rmd** that generates the file **dataset_after_cleaning.csv**\n\nIn this script we will continue to prepare our dataset for use in meta-analysis, calculates variables for effect size estimation (SMDH, lnRR).\n\n#### Input files needed\n\ndata/03_data_cleaning/dataset_after_cleaning.csv\n\n#### Output files generated\n\n### Setting up the workspace\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n# Setting up workspace\nif (!requireNamespace(\"pacman\", quietly = TRUE)) {\n  install.packages(\"pacman\")\n}\npacman::p_load(here, tidyverse, dplyr, stringr, metafor, orchaRd,ggpubr,ggrepel) #Packages needed\n\nrm(list=ls()) ## cleaning up\n\nset.seed(8341) #set seed for reproducibility (for random number generation)\n\n# Loading cleaned dataset\n\ndataset_after_cleaning <- read_csv(here::here(\"data/03_data_cleaning/dataset_after_cleaning.csv\"))\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nRows: 283 Columns: 53\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (30): paper_ID, fulltext_screening, variable_note, authors, population_l...\ndbl (22): year_publication, Observation_ID, experiment_ID, group_ID, repeate...\nlgl  (1): fulltext_notes\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\n## Now all the changes we make to the file.. we will store them in data_analysis\n\ndataset_analysis<-dataset_after_cleaning\n```\n:::\n\n\n#### Loading functions\n\nSome functions we will need in this manuscript.. sourced mainly from Nakagawa 2022 : Online tutorial <https://alistairmcnairsenior.github.io/Miss_SD_Sim/>\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n# Function to calculate Geary's \"number\"\n  geary <- function(mean, sd, n){\n    (1 / (sd / mean)) * ((4*n^(3/2)) / (1 + 4*n))\n  }\n\n  \n\n# Shinichi Nakagawa, Daniel W. A. Noble, Malgorzata Lagisz, Rebecca Spake, Wolfgang Viechtbauer and Alistair M. Senior. 2022. A robust and readily implementable method for the meta-analysis of response ratios with and without missing standard deviations. Ecology Letters, DOI: 10.1111/ele.14144\n\n\n  # Functions provided in the workalong\ncv_avg <- function(x, sd, n, group, data, label = NULL, sub_b = TRUE, cv2=FALSE){\n\n  # Check if the name is specified or not. If not, then assign it the name of the mean, x, variable input in the function. https://stackoverflow.com/questions/60644445/converting-tidyeval-arguments-to-string\n  if(is.null(label)){\n    label <- purrr::map_chr(enquos(x), rlang::as_label)\n  }\n\n  # Calculate between study CV. Take weighted mean CV within study, and then take a weighted mean across studies of the within study CV. Weighted based on sample size and pooled sample size.\n  b_grp_cv_data <- data                                             %>%\n    dplyr::group_by({{group}})                            %>%\n    dplyr::mutate(   w_CV2 = weighted_CV({{sd}}, {{x}}, {{n}}, cv2=cv2),\n                     n_mean = mean({{n}}, na.rm = TRUE))   %>%\n    dplyr::ungroup(.)                                     %>%\n    dplyr::mutate(b_CV2 = weighted.mean(w_CV2, n_mean, na.rm = TRUE), .keep = \"used\")\n\n  # Make sure that label of the calculated columns is distinct from any other columns\n  names(b_grp_cv_data) <- paste0(names(b_grp_cv_data), \"_\", label)\n\n  # Append these calculated columns back to the original data and return the full dataset.\n  if(sub_b){\n    b_grp_cv_data <- b_grp_cv_data %>% dplyr::select(grep(\"b_\", names(b_grp_cv_data)))\n    dat_new <- cbind(data, b_grp_cv_data)\n  } else {\n    dat_new <- cbind(data, b_grp_cv_data)\n  }\n\n  return(data.frame(dat_new))\n}\n\n# You also need the helper function\n\nweighted_CV <- function(sd, x, n, cv2=FALSE){\n  if(cv2){\n    weighted.mean(na_if((sd / x)^2, Inf), n, na.rm = TRUE)\n  }else{\n    weighted.mean(na_if((sd / x), Inf), n, na.rm = TRUE)^2\n  }\n}\n\n\n#' @title lnrr_laj\n#' @description Calculates log response ratio based on Taylor expansion from Jajeunesse 2011\n#' @param m1 Mean of treatment group 1\n#' @param m2 Mean of treatment group 2\n#' @param cv1_2 Coefficient of variation squared (CV^2) for treatment group 1\n#' @param cv2_2 Coefficient of variation squared (CV^2) for treatment group 2\n#' @param n1 Sample size for treatment group 1\n#' @param n2 Sample size for treatment group 2\n#' @param taylor A logical indicating whether to calculate point estimate with Taylor expansion.\n#'\nlnrr_laj <- function(m1, m2, cv1_2, cv2_2, n1, n2, taylor=TRUE){\n  if(taylor){\n    log(m1 / m2) + 0.5*((cv1_2 / n1) - (cv2_2 / n2))\n  } else {\n    log(m1 / m2)\n  }\n}\n\n#' @title v_lnrr_laj\n#' @description Calculates the sampling variance for log response ratio based on second order Taylor expansion proposed by Lajeunesse 2011\n#' @param cv1_2 Coefficient of variation squared (CV^2) for treatment group 1\n#' @param cv2_2 Coefficient of variation squared (CV^2) for treatment group 2\n#' @param n1 Sample size for treatment group 1\n#' @param n2 Sample size for treatment group 2\n#' @param taylor A logical indicating whether to calculate point estimate with Taylor expansion.\nv_lnrr_laj <- function(cv1_2, cv2_2, n1, n2,  taylor=TRUE){\n  if(taylor){\n  ((cv1_2) / n1) + ((cv2_2) / n2) +\n    ((cv1_2)^2 / (2*n1^2)) + ((cv2_2)^2 / (2*n2^2))\n  } else {\n    ((cv1_2) / n1) + ((cv2_2) / n2)\n  }\n}\n```\n:::\n\n\n# Preparing Random Effects\n\nIn our pre-registration, we considered the following levels of non-independence for our analysis:\n\n\"(1) paper_ID, which encompasses estimates extracted from the same primary study,\n\n(2) experiment_ID,\n\n(3) group_ID\n\n(4) repeated_traits_ID, and\n\n(5) observation_ID which corresponds to a unit-level observation identity and models within-study variance\n\nAs mentioned above, if we obtain additional effect sizes for other bird species after we update our search and/or contact authors, our models may need to account for phylogenetic nonindependence via the inclusion of two additional random effects: \"species\" and \"phylogeny\" following Cinar et al. (2022).\"\n\n-   We also need to add `observation ID` which is the unit-level identity (unique for each row) for within-study variance.\n\n-   In the way we extracted this information, each paper_ID needs to be added to the random effect level of `experiment_ID`, `group_ID` and `repeated_trait_ID` to make sure they are unique values.\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\ndataset_analysis<-dataset_analysis%>%\n  \n  mutate(Observation_ID = as.factor(row_number()))%>%\n  \n  mutate(experiment_ID_coded = as.factor(paste(paper_ID, experiment_ID, sep=\"_\")),.after=experiment_ID)%>%\n  mutate(group_ID_coded = as.factor(paste(paper_ID, group_ID, sep=\"_\")),.after=group_ID)%>%\n  mutate(repeated_trait_ID_coded = as.factor(paste(paper_ID,repeated_trait_ID, sep=\"_\")),.after=repeated_trait_ID)\n\n# random_effects_check<-dataset_analysis%>%select(paper_ID,experiment_ID,group_ID,repeated_trait_ID, experiment_ID_coded,group_ID_coded, repeated_trait_ID_coded)\n```\n:::\n\n\n-   We also decided to add `population ID` since we noticed several studies were performed in the same geographical location.\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\ndataset_analysis<-dataset_analysis%>%\n  mutate(population_ID = as.factor(population_location))\n\n# Let's look at the nammes of the locations\n# unique(dataset_analysis$population_ID)\n\n# There are clearly many repeated names, let's clean this up a bit\n\ndataset_analysis <- dataset_analysis %>%\n  mutate(\n    population_ID = str_squish(as.character(population_location)),  # Clean whitespace\n    population_ID = case_when(\n      # Lodz\n      str_detect(population_ID, regex(\"lodz|łódź|central poland\", ignore_case = TRUE)) ~ \"Lodz, Poland\",\n      \n      # Montijo / Tagus Estuary\n      str_detect(population_ID, regex(\"montijo|tagus estuary|air force base\", ignore_case = TRUE)) ~ \"Montijo, Portugal\",\n      \n      # Madrid Region\n      str_detect(population_ID, regex(\"madrid|manzanares|villalba|soto del real|central spain\", ignore_case = TRUE)) ~ \"Madrid Province, Spain\",\n      \n      # Nova Scotia / New Brunswick\n      str_detect(population_ID, regex(\"nova scotia|new brunswick\", ignore_case = TRUE)) ~ \"Nova Scotia and New Brunswick, Canada\",\n      \n      # Vosbergen\n      str_detect(population_ID, regex(\"vosbergen|groningen\", ignore_case = TRUE)) ~ \"Vosbergen, Netherlands\",\n      \n      # Hoya de Guadix\n      str_detect(population_ID, regex(\"hoya de guadix|calahorra|hueneja\", ignore_case = TRUE)) ~ \"Hoya de Guadix, Spain\",\n      \n      # Valsaín\n      str_detect(population_ID, regex(\"valsain|segovia\", ignore_case = TRUE)) ~ \"Valsaín, Spain\",\n      \n      # Vanderhoof\n      str_detect(population_ID, regex(\"vanderhoof\", ignore_case = TRUE)) ~ \"Vanderhoof, Canada\",\n      \n      # Patuxent\n      str_detect(population_ID, regex(\"patuxent|laurel, maryland\", ignore_case = TRUE)) ~ \"Patuxent Wildlife Research Center, USA\",\n      \n      # Betty Daw’s Wood\n      str_detect(population_ID, regex(\"betty daw\", ignore_case = TRUE)) ~ \"Betty Daw’s Wood, UK\",\n      \n      # Lake Ammersee\n      str_detect(population_ID, regex(\"ammersee|south germany|andechs\", ignore_case = TRUE)) ~ \"Lake Ammersee, Germany\",\n      \n      # Corsica\n      str_detect(population_ID, regex(\"corsica|muro valley|quercus\", ignore_case = TRUE)) ~ \"Corsica, France\",\n      \n      # SWRC Philadelphia\n      str_detect(population_ID, regex(\"swrc|philadelphia\", ignore_case = TRUE)) ~ \"SWRC, Philadelphia, USA\",\n      \n      # Coimbra\n      str_detect(population_ID, regex(\"coimbra|mata nacional do choupal\", ignore_case = TRUE)) ~ \"Coimbra, Portugal\",\n      \n      # Kuankuoshui\n      str_detect(population_ID, regex(\"kuankuoshui\", ignore_case = TRUE)) ~ \"Kuankuoshui Nature Reserve, China\",\n      \n      # Werther\n      str_detect(population_ID, regex(\"werther|52°06′n|52°06’n\", ignore_case = TRUE)) ~ \"Werther, Germany\",\n      \n      # Bergen\n      str_detect(population_ID, regex(\"bergen\", ignore_case = TRUE)) ~ \"Bergen, Norway\",\n      \n      # for clean but unmatched names\n      TRUE ~ str_to_title(population_ID)\n    )\n  )\n```\n:::\n\n\n# Effective Sample Size Calculation\n\nFor comparisons involving the shared control or treatment groups across multiple studies, we adjust the sample sizes by dividing them by the number of times each group is included in comparisons. This adjustment helps to prevent any inflation of the effect sizes due to repeated use of the same groups and account for the shared-group non-independence.\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\ndataset_analysis<-dataset_analysis%>%\n  mutate(effective_n_experiment=(n_experiment/shared_experiment),.after=n_experiment)%>%\n  mutate(effective_n_control=(n_control/shared_control),.after=n_control)\n```\n:::\n\n\nChecking the range of effective sample size in our dataset\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\ndataset_analysis%>%\n  summarise(control_min=min(effective_n_control,na.rm=T),\n            control_max=max(effective_n_control,na.rm=T),\n            na_control = sum(is.na(effective_n_control)),\n            experiment_min=min(effective_n_experiment,na.rm=T),\n            experiment_max=max(effective_n_experiment,na.rm=T),\n             na_experiment = sum(is.na(effective_n_experiment)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 6\n  control_min control_max na_control experiment_min experiment_max na_experiment\n        <dbl>       <dbl>      <int>          <dbl>          <dbl>         <int>\n1           1          99          0              1            111             0\n```\n\n\n:::\n:::\n\n\nWe have some NA and some values of effective sample size 1 and below.. lets see where this is\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nlow_sample<-dataset_analysis%>%\n  filter(effective_n_control<2 | effective_n_experiment<2)\n```\n:::\n\n\nMany of these data points come from GNM_355. There are datapoints with sample size of 1 for some groups. We have extracted data by days and months.. I will combine these for this paper because of such low sample size.\n\nAfter combining or those I could not combine.. some still have a low sample size..\n\nWe can only estimate SMHD with dF (n1+n2-2) greater than 1. lnRR could be calculated regardless. So I will not exclude any rows specifically even though the effective sample size could be lower than 2 in a group.\n\n# Converting variance to SD\n\nThere were no other measure of dispersion, only SE and SD, so no need to convert anything more.\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n## Calculating sd from difference variance measures (only for se so far)\n\n# For experimental group\ndataset_analysis <- dataset_analysis %>%\n  mutate(sd_experiment = if_else(type_measure_dispersion_experiment %in% c('se', 'SE'), #condition to check\n      measure_dispersion_experiment * sqrt(as.numeric(n_experiment)), # do this if true\n      measure_dispersion_experiment), #else do this\n      .after=measure_dispersion_experiment)\n# For control group\ndataset_analysis <- dataset_analysis %>%\n  mutate(sd_control = if_else(type_measure_dispersion_control %in% c('se', 'SE'), #condition to check\n      measure_dispersion_control* sqrt(as.numeric(n_control)), # do this if true\n      measure_dispersion_control), #else do this\n      .after=measure_dispersion_control)\n```\n:::\n\n\nSome checks for the value of mean and SD\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n# Some rows contain NAs, check why\n# contains_NA<-dataset_analysis%>%\n# filter(is.na(measure_central_tendency_experiment)|is.na(measure_dispersion_experiment))\n\n# 24 rows contain NA and I checked them manually, everything looks okay. \n# - 4 rows contain only statistical test values and \n# - 4 rows contain values for contingency tables only\n# - 16 are missing data that authors did not provide for which we will use 0 as effect size. \n```\n:::\n\n\n# No effect size provided\n\nAuthors did not report the means/effect size for some cases but said they found no difference between the group. For these cases, we will assume the effect size to be exactly zero.\n\nWe will calculate the mean trait values by trait type (e.g. physiological, reproductive, etc.) We calculate the mean of both treatment and control group. Then I will assign this mean trait value to all the fitness proxies of that trait type. This way, the difference X1-X2 would be 0 (for SMDH) as well as the log(X1/X2) would also be zero (for log response ratio). Using this, we will then impute the SD for the effect size.\n\nWe imputed its sampling variance using the missing-case approach described for lnRR. (see *Nakagawa, S., Noble, D.W.A., Lagisz, M., Spake, R., Viechtbauer, W. & Senior, A.M. (2023) A robust and readily implementable method for the meta-analysis of response ratios with and without missing standard deviations. Ecology Letters, 26, 232–244. Available from: <https://doi.org/10.1111/ele.14144>* for more details)\n\nUsing the formula (n1+n2)/(n1×n2)(n_1 + n_2)/(n_1 \\\\times n_2)(n1​+n2​)/(n1​×n2​) for SMDH (see *Lajeunesse, Marc J., 'Recovering Missing or Partial Data from Studies: A Survey of Conversions and Imputations for Meta-analysis', in Julia Koricheva, Jessica Gurevitch, and Kerrie Mengersen (eds), Handbook of Meta-analysis in Ecology and Evolution (Princeton, NJ, 2013; online edn, Princeton Scholarship Online, 19 Oct. 2017), <https://doi.org/10.23943/princeton/9780691137285.003.0013>, accessed 5 July 2025* for more details)\\\n\\\nWe will also run a sensitivity analysis without these proxies to ensure robustness of our results.\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n# To calculate means by groups\ntrait_mean<- dataset_analysis%>%\n  group_by(trait_type)%>%\n  summarise(\n    cumulative_mean = mean (\n      c(measure_central_tendency_experiment,measure_central_tendency_control),\n      na.rm=T\n    ),\n    count = n()\n  )%>%\n  arrange(desc(count))\n\n# Now I need to assign the mean values from this to the missing ES proxies\ndataset_analysis<-dataset_analysis%>%\n  left_join(trait_mean, by= \"trait_type\")%>%\n  mutate(measure_central_tendency_experiment = if_else((\n    proxy_comment == \"use 0 as ES\" & !is.na(proxy_comment)),\n    cumulative_mean,\n    measure_central_tendency_experiment),\n    measure_central_tendency_control = if_else((\n    proxy_comment == \"use 0 as ES\" & !is.na(proxy_comment)),\n    cumulative_mean,\n    measure_central_tendency_control),)%>%\n  select(-cumulative_mean)\n```\n:::\n\n\n# Effect Size Estimation\n\n## Calculate lnRR\n\n### Geary's Test\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n# Assumption of normality assumed to be approximately correct when values are >= 3.\ndataset_analysis<- dataset_analysis %>% \n         mutate(geary_control = geary(measure_central_tendency_control,\n                                      sd_control, \n                                      n_control),\n                    geary_trt = geary(measure_central_tendency_experiment,\n                                      sd_experiment, \n                                      n_experiment),\n                   geary_test = ifelse(geary_control >= 3 & geary_trt >= 3, \"pass\", \"fail\"))\n\n# How many fail?\n    geary_res <-dataset_analysis %>% group_by(geary_test) %>% summarise(n = n()) %>%  data.frame()\n    \n    geary_res\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  geary_test   n\n1       fail  48\n2       pass 211\n3       <NA>  24\n```\n\n\n:::\n:::\n\n\n**48 observations fail Geary's test.**\n\nHowever, we do not exclude anything but as suggested by Lajeunesse (2015), I will do a sensitivity analysis with and without these effect sizes for lnRR.\n\nSince lnRR can only be calculated for ratio scale data (i.e., data with a true zero; among other assumptions), we will exclude effect sizes with a negative value when calculating lnRR.\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n## ─────────────────────────────────────────────────────────────\n## 1.  Keeping rows where lnRR can be calculated. \n## Inferential stats and contingency table can only be used for SMDH.\n##    — store the dropped rows to inspect and join back later\n## ─────────────────────────────────────────────────────────────\n\nonly_SMDH <-dataset_analysis%>%\n  filter(proxy_decision == \"contingency table\"\n         |proxy_decision == \"SMDH only\")\n\ndataset_analysis <- dataset_analysis %>%\n  anti_join(only_SMDH, by = names(dataset_analysis))\n\n## -------------------------------------------------------------------------\n## 2. Rows whose means are > 0 (lnRR needs ratio scale data)\n##  - store the dropped rows to inspect and join back later to the dataset\n## -------------------------------------------------------------------------\n\nneg_means<- dataset_analysis%>%\n filter(measure_central_tendency_experiment <= 0 |\n         measure_central_tendency_control <= 0)\n\ndataset_analysis <- dataset_analysis %>%\n  anti_join(neg_means, by = names(dataset_analysis))\n\n## -------------------------------------------------------------------------\n## 3. Estimating lnRR using metafor::escalc for cases where SD is available\n## -------------------------------------------------------------------------\n\n# Using function 'escalc()' (with measure=ROM and 'vtype=LS')\n\ndataset_analysis<- escalc(measure = \"ROM\", \n              vtype= \"LS\",\n                        n1i = effective_n_experiment, \n                        n2i = effective_n_control,\n                        m1i = measure_central_tendency_experiment, \n                        m2i = measure_central_tendency_control,\n                        sd1i = sd_experiment, \n                        sd2i = sd_control,\n                        data = dataset_analysis, \n                        var.names = c('lnRR',\n                                      'lnRR_variance'),\n                        add.measure = FALSE,\n                        append = TRUE)\n\n## How many lnRR were not calculates?\n\nnot_calculated<-dataset_analysis%>%filter(is.na(lnRR)|is.na(lnRR_variance))\n\n## 16 rows have missing values of ES.. we have to impute the values here..\n\n## -------------------------------------------------------------------------\n## 4. Estimating lnRR using Missing Case approach from Nakagawa 2022\n##    see https://alistairmcnairsenior.github.io/Miss_SD_Sim/\n## -------------------------------------------------------------------------\n\n# A: Calculate the average between study CV, which will replace missing values.\n   # Study-level pooled CV² (square of weighted-average CV)\n   # – one call per group, using your cv_avg() helper\n\ndataset_analysis <- cv_avg(x = measure_central_tendency_experiment, \n                           sd = sd_experiment,\n                           n = effective_n_experiment, \n                           group = paper_ID, \n                           label = \"exp\",\n                           data = dataset_analysis)\ndataset_analysis <- cv_avg(measure_central_tendency_control, \n                           sd = sd_control,\n                           n = effective_n_control, \n                           group = paper_ID, \n                           label = \"con\",\n                           data = dataset_analysis)\n# B: Keep study-specific CV² when SD present and pooled CV² when SD is missing\n\n\ndataset_analysis <- dataset_analysis %>%\n  mutate(cv_Control= sd_control/measure_central_tendency_control,\n         cv_Experimental= sd_experiment/measure_central_tendency_experiment,\n         cv2_con_new = if_else(is.na(sd_control),\n                               b_CV2_con,          # pooled CV²\n                               cv_Control^2),      # study-specific CV²\n         cv2_exp_new = if_else(is.na(sd_experiment),\n                               b_CV2_exp,\n                               cv_Experimental^2))\n\n\n# C: Lajeunesse Eqs. 6 & 7 for EVERY row using lnrr_laj and v_lnrr_laj functions\n## As would be recommended for ALL CASE method from Nakagawa 2022. \n\ndataset_analysis <- dataset_analysis %>%\n  mutate(lnrr_laj = lnrr_laj(m1 = measure_central_tendency_experiment,\n                             m2 = measure_central_tendency_control,\n                             cv1_2 = cv2_exp_new,\n                             cv2_2 = cv2_con_new,\n                             n1 = effective_n_experiment,\n                             n2 = effective_n_control),\n         v_lnrr_laj = v_lnrr_laj(cv1_2 = cv2_exp_new,\n                                 cv2_2 = cv2_con_new,\n                                 n1 = effective_n_experiment,\n                                 n2 = effective_n_control))\n\n\n## -------------------------------------------------------------------------\n## 4. Replace NA (\"use 0 as ES\") cases in lnRR/lnRR_variance with imputed values\n## -------------------------------------------------------------------------\n\n\ndataset_analysis<-dataset_analysis%>%\n  mutate(lnRR = if_else((\n    proxy_comment == \"use 0 as ES\" & !is.na(proxy_comment)),\n    lnrr_laj,\n    lnRR),\n   lnRR_variance = if_else((\n    proxy_comment == \"use 0 as ES\" & !is.na(proxy_comment)),\n    v_lnrr_laj,\n    lnRR_variance),)%>%\nselect(-b_CV2_exp,-b_CV2_con,-cv_Control,-cv_Experimental,-cv2_con_new,-cv2_exp_new,-lnrr_laj,-v_lnrr_laj)\n\n\n## How many lnRR were not calculated now?\n\nnot_calculated<-dataset_analysis%>%filter(is.na(lnRR)|is.na(lnRR_variance))\n\n## All rows have a value, good.. this is what would be expected.\n\n\n## -------------------------------------------------------------------------\n## 5. Adding back the removed rows with NA values for the new columns\n## We still have to use these for calculating SMDH values in the dataset\n## -------------------------------------------------------------------------\n\ndataset_analysis<-bind_rows(dataset_analysis,only_SMDH,neg_means)\n```\n:::\n\n\n### Sign for Effect Size\n\nWe have to ensure throughout the dataset that positive values mean higher estimates of fitness in the experimental group compared to their control counterparts by multiplying all the effect sizes by their corresponding sign **(stored in variable proxies_sign)**, i.e., by -1 or +1.\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\ndataset_analysis<-dataset_analysis%>%\n  mutate(lnRR_sign=lnRR*proxies_sign)\n```\n:::\n\n\n### Outliers Check\n\nWe do not plan to exclude any outliers as long as the extracted data is seemingly correct.\n\nNow let's plot means and SD to see everything looks okay\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n# ## We have to exclude the datapoints where we do not have dispersion value. We have added 0 ES in some cases and this is not allowing the lm line to be plotted\n# #\n# temp_data<-dataset_analysis%>%\n#   filter(!is.na(measure_dispersion_experiment) & !is.na(measure_dispersion_control) & !is.na(measure_central_tendency_experiment) & !is.na(measure_central_tendency_control) & !is.na(lnRR))\n# dataset_lnRR_plot<-temp_data\n# \n# outliers_experiment<- dataset_lnRR %>%\n#   filter(measure_dispersion_experiment>100)\n# outliers_control<- dataset_lnRR %>%\n#   filter(measure_dispersion_control >100)\n# \n# \n# # For experimental group\n# \n# # Fit a linear model to extract the intercept\n# lm_fit_experiment <- lm(log(measure_dispersion_experiment) ~\n#                           log(measure_central_tendency_experiment),\n#                         data = dataset_lnRR_plot)\n# intercept_value_experiment <- coef(lm_fit_experiment)[1] # Extract the intercept\n# \n# lm_fit_control <- lm(log(measure_dispersion_control) ~\n#                        log(measure_central_tendency_control),\n#                      data = dataset_lnRR_plot)\n# intercept_value_control <- coef(lm_fit_control)[1]\n# \n# # Scatter plot of means vs SDs for experimental group\n# dataset_lnRR_plot %>%\n#   ggplot(aes(x = log(measure_central_tendency_experiment),\n#              y = log(measure_dispersion_experiment),\n#              size = effective_n_experiment,\n#              colour = trait_type)) +\n#   geom_point(alpha = 0.6) +\n#   geom_text_repel(\n#     data = outliers_experiment,\n#     aes(\n#       x = log(measure_central_tendency_experiment),\n#       y = log(measure_dispersion_experiment),\n#       label = fitness_proxy_cleaned\n#     ),\n#     size = 3, color = \"black\", max.overlaps = 15\n#   ) +\n#   geom_smooth(\n#     method = \"lm\",  # Adds a regression line\n#     se = TRUE,      # Adds confidence intervals\n#     color = \"blue\", # Color of the line\n#     linetype = \"solid\",# Style of the line\n#     size = 0.4\n#   ) +\n#   # Add a line representing correlation of 1\n#   geom_abline(\n#     slope = 1,       # Slope of the line\n#     intercept = intercept_value_experiment,   # Intercept of the line\n#     color = \"black\", # Color of the line\n#     linetype = \"dashed\", # Style of the line\n#     size = 0.4       # Thickness of the line\n#   )+\n#   labs(\n#     x = \"Log Mean Value\",\n#     y = \"Log Standard Deviation (SD)\",\n#     title = \"Log-transformed Experimental Groups\"\n#   ) +\n#   theme_minimal() +\n#   theme(\n#     plot.title = element_text(size = 12, face = \"bold\"),\n#     axis.title = element_text(size = 10),\n#     axis.text = element_text(size = 10)\n#   )\n# \n# # For Control group\n# \n# # Scatter plot of means vs SDs for experimental group\n# dataset_lnRR_plot %>%\n#   ggplot(aes(x = log(measure_central_tendency_control),\n#              y = log(measure_dispersion_control),\n#              size = effective_n_control)) +\n#   geom_point(color = \"brown\", alpha = 0.6) +\n#   geom_text_repel(\n#     data = outliers_control,\n#     aes(\n#       x = log(measure_central_tendency_control),\n#       y = log(measure_dispersion_control),\n#       label = fitness_proxy_cleaned\n#     ),\n#     size = 3, color = \"black\", max.overlaps = 15\n#   ) +\n#   geom_smooth(\n#     method = \"lm\",  # Adds a regression line\n#     se = TRUE,      # Adds confidence intervals\n#     color = \"blue\", # Color of the line\n#     linetype = \"dashed\", # Style of the line\n#     size= 0.4\n#   ) +\n#   # Add a line representing correlation of 1\n#   geom_abline(\n#     slope = 1,       # Slope of the line\n#     intercept = intercept_value_control,   # Intercept of the line\n#     color = \"black\", # Color of the line\n#     linetype = \"dashed\", # Style of the line\n#     size = 0.4       # Thickness of the line\n#   )+\n#   labs(\n#     x = \"Log Mean Value\",\n#     y = \"Log Standard Deviation (SD)\",\n#     title = \"Log-transformed Control Groups\"\n#   ) +\n#   theme_minimal() +\n#   theme(\n#     plot.title = element_text(size = 12, face = \"bold\"),\n#     axis.title = element_text(size = 10),\n#     axis.text = element_text(size = 10),\n#     legend.position = \"none\"\n#   )\n# \n# \n# # ggarrange(sd_experiment_plot,sd_control_plot,\n#                                      ncol = 2,  nrow = 1,\n#                                      labels = c(\"A.\", \"B.\",\n#                                                 common.legend = TRUE))\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nfunnel_lnRR <- dataset_analysis %>%\n  ggplot(aes(x = lnRR_sign, y = 1 / lnRR_variance)) +\n  geom_point(color= \"#7DCE82\", alpha = 0.4, size = 3) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"#8B4513\", linewidth=0.65) +  # Add vertical dashed line at x = 0\n  ggtitle(\"Funnel Plot for lnRR\") +\n  xlab(\"lnRR\") +\n  ylab(\"Precision (1/Variance)\")\n\nprint(funnel_lnRR)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nWarning: Removed 15 rows containing missing values or values outside the scale range\n(`geom_point()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](06_data_preparation_files/figure-html/Funnel Plot-1.png){width=672}\n:::\n\n```{.r .cell-code .hidden}\nrows_with_missing <- dataset_analysis %>%\n  filter(is.na(lnRR_sign) | is.na(lnRR_variance))\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nlog_values <- dataset_analysis %>%\n  filter(str_detect(variable_note, regex(\"log\", ignore_case = TRUE)))\n```\n:::\n\n\n## Calculate SMDH\n\n-   For results only reported as inferential statistics (e.g., t-test, chi-square), we will use the equations provided by (Lajeunesse 2013; Nakagawa and Cuthill 2007; and other sources if needed) to calculate their corresponding SMD estimates for the analyses.\n\n-   Note that lnRR cannot be calculated from inferential statistics and that only SMD without heteroskedasticity correction can be calculated from inferential statistics.\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n#-------------------------------------------------------------------------\n# 1. SMDH for most cases where mean, variance and sample size was reported\n#-------------------------------------------------------------------------\n\n## using function 'escalc()' with measure=SMDH and 'vtype=LS'\n\ndataset_analysis<- escalc(measure = \"SMD\", \n              vtype= \"LS\",\n                        n1i = effective_n_experiment, \n                        n2i = effective_n_control,\n                        m1i = measure_central_tendency_experiment, \n                        m2i = measure_central_tendency_control,\n                        sd1i = sd_experiment, \n                        sd2i = sd_control,\n                        data = dataset_analysis, \n                        var.names = c('SMDH',\n                                      'SMDH_variance'),\n                        add.measure = FALSE,\n                        append = TRUE)\n\n# Cases where it was not calculated using mean, variance and sample size\nnoSMDH<-dataset_analysis%>%filter(is.na(SMDH))\n\n#-------------------------------------------------------------------------\n# 2. No effect of the treatment is present but \n# estimates (means, test statistics or effect size) were not reported\n#-------------------------------------------------------------------------\n\n# Since authors report no significant effect of the treatment, \n# we assume that the means are not different in the treatment and control group. \n# Under this assumption SMD = M(treatment)-M(control)/SD(pooled) which would be zero\n# and the associated variance = n(treatment)+n(control)/n(treatment)+n(control)\n\ndataset_analysis <- dataset_analysis %>%\n  mutate(SMDH = if_else(proxy_comment == \"use 0 as ES\" & !is.na(proxy_comment), (measure_central_tendency_experiment-measure_central_tendency_control), SMDH))%>%\n  mutate(SMDH_variance = if_else(proxy_comment == \"use 0 as ES\" & !is.na(proxy_comment), ((effective_n_experiment+effective_n_control)/(effective_n_experiment*effective_n_control)), SMDH_variance))\n\n# Cases where it was not calculated using mean, variance and sample size\nnoSMDH<-dataset_analysis%>%filter(is.na(SMDH))\n```\n:::\n\n\n#### Data as contingency table\n\n**IMPORTANT NOTE**: For calculation from contingency tables, the measure of central tendency is used to store value of percentage of success reported. It is not a measure of central tendency.\n\nFor cases when the data is reported as a contingency table. In these cases we will calculate the Odds ratio and convert it to SMD.. (and not SMDH)\n\nGeneral Calculation Formulas\n\n1.  **Number of Successes and Failures**\n\n    -   Success in Group A: $S_A = \\left(\\frac{p_A}{100}\\right) \\times N_A$\n\n    -   Failure in Group A: $F_A = N_A - S_A$\n\n    -   Success in Group B: $S_B = \\left(\\frac{p_B}{100}\\right) \\times N_B$\n\n    -   Failure in Group B: $F_B = N_B - S_B$\n\n        where N is Total Sample Size and P is the number of successful nests\n\n2.  **Odds Ratio (OR)**\\\n    $$OR = \\frac{S_A \\times F_B}{S_B \\times F_A}$$\n\n3.  **Log Odds Ratio (lnOR)**\\\n    $$\\ln(OR) = \\log \\left( \\frac{S_A \\times F_B}{S_B \\times F_A} \\right)$$\n\n4.  **Variance of Log Odds Ratio (lnOR)**\n\n    $$ V_{\\ln OR} = \\frac{1}{2} \\left( \\frac{1}{S_A} + \\frac{1}{F_A} + \\frac{1}{S_B} + \\frac{1}{F_B} \\right)$$\n\n5.  **Standardized Mean Difference (SMD)**\n\n    $$SMD = \\frac{\\ln(OR) \\times \\sqrt{3}}{\\pi}$$\n\n6.  **Variance of Standardized Mean Difference (SMD)**\n\n    $$V_{SMD} = \\frac{3 \\times V_{\\ln OR}}{\\pi^2}$$\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\ncontingency_table<-dataset_analysis%>%filter(proxy_decision==\"contingency table\")\n\nunique(contingency_table$fitness_proxy)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"nests with complete reproductive success (%)\"\n[2] \"Percent Survived\"                            \n```\n\n\n:::\n\n```{.r .cell-code .hidden}\n# We have nests with complete reproductive success (%)..\n\n# Calculate Odds Ratio (OR) and Log Odds Ratio (lnOR)\n\ncontingency_table <- contingency_table %>%\n  mutate(\n    # Calculate the number of successes and failures for experiment and control groups\n    success_experiment = round((measure_central_tendency_experiment / 100) * n_experiment),\n    failure_experiment = n_experiment - success_experiment,\n    success_control = round((measure_central_tendency_control / 100) * n_control),\n    failure_control = n_control - success_control)\n    # Calculate Log Odds Ratio (lnOR)\ncontingency_table<- escalc(measure=\"OR\", ai=success_experiment, bi=failure_experiment, ci=success_control, di=failure_control, data=contingency_table, var.names = c('lnOR',\n                                      'lnOR_variance'))\n# Calculate SMDH\ncontingency_table <- contingency_table %>%\n  mutate(SMDH=(lnOR*sqrt(3))/pi,\n         SMDH_variance=(3*lnOR_variance)/pi^2)\n\n# Update SMDH and SMDH_variance based on Observation_ID\ndataset_analysis <- rows_update(\n  x = dataset_analysis,           # Target dataset to update\n  y = contingency_table %>% \n        select(Observation_ID, SMDH, SMDH_variance), # Updates from contingency_table\n  by = \"Observation_ID\"       # Match on Observation_ID\n)\n```\n:::\n\n\n#### For SMDH from ANCOVA and t-test\n\nNote that, since we did not have access to the corresponding $R^2$ values, using Equation 1 for transforming F-values from an ANCOVA into Cohen's d, we will use Equation 2 instead.\n\nEquation 1: Estimating Cohen's d from independent groups using ANCOVA\n\n![From Cooper2019 Table 11.3](/images/ancova-d.png)\n\nEquation 2: Estimating Cohen's d from independent groups using ANOVA\n\n![From Cooper2019 Table 11.3](/images/anova-d.png)\n\nWe will slightly overestimate $d$ and underestimate $V_d$ the larger that unknown $R^2$ value is (see simulation on sensitivity analysis tab); however, we decided to use it to avoid excluding those three effect sizes.\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nancova_SMDH<-dataset_analysis%>%filter(statistics_type==\"ANCOVA\")\n# 3 effect size where SMDH needs to be estimated from ANCOVA \n\n# Function to compute Cohen's d (from ANOVA)\nd_anova <- function(F, n1, n2) {\n  sqrt((F * (n1 + n2)) / (n1 * n2))\n}\n\n# Function to compute variance (from ANOVA)\nvar_d_anova <- function(d_anova, n1, n2) {\n  ((n1 + n2) / (n1 * n2)) + (d_anova^2 / (2 * (n1 + n2)))\n}\n\nancova_SMDH<-ancova_SMDH%>%\n  mutate(SMDH = sign_relationship*(d_anova(statistics_value,n_experiment,n_control)))%>%\n    mutate(SMDH_variance = var_d_anova(SMDH, n_experiment,n_control))\n\n# Update SMDH and SMDH_variance based on Observation_ID\ndataset_analysis <- rows_update(\n  x = dataset_analysis,           # Target dataset to update\n  y = ancova_SMDH %>% \n        select(Observation_ID, SMDH, SMDH_variance), # Updates from ancova_SMDH\n  by = \"Observation_ID\"       # Match on Observation_ID\n)\n```\n:::\n\n\n![from Table 11.1 Cooper2019](/images/t-d.png)\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n# 1 effect size where SMDH needs to be estimated from t-test statistics\nt_test_SMDH<-dataset_analysis%>%filter(statistics_type==\"t-test\")\n\n# Function to compute Cohen's d (from t)\nd_t_test <- function(t, n1, n2) {\n  t * sqrt((n1 + n2) / (n1 * n2))\n}\n\n# Function to compute variance (from t)\nvar_d_t_test <- function(d_t_test, n1, n2) {\n  ((n1 + n2) / (n1 * n2)) + (d_t_test^2 / (2 * (n1 + n2)))\n}\n\nt_test_SMDH<-t_test_SMDH%>%\n  mutate(SMDH = sign_relationship*(d_t_test(statistics_value,n_experiment,n_control)))%>%\n    mutate(SMDH_variance = var_d_t_test(SMDH, n_experiment,n_control))\n\n# Update SMDH and SMDH_variance based on Observation_ID\ndataset_analysis <- rows_update(\n  x = dataset_analysis,           # Target dataset to update\n  y = t_test_SMDH %>% \n        select(Observation_ID, SMDH, SMDH_variance), # Updates from t_test_SMDH\n  by = \"Observation_ID\"       # Match on Observation_ID\n)\n```\n:::\n\n\n### Sign for Effect Size\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\ndataset_analysis<-dataset_analysis%>%\n  mutate(SMDH_sign=SMDH*proxies_sign)\n```\n:::\n\n\n### Low sample size\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n# 9 from GNM_355 where repeated estimates over days for haemoglobin are present and I cannot combine them as I did for chick mass over months.. since the sample is the same and these are the repeated measures of the same sample.\nlow_sample=dataset_analysis%>%filter(is.na(SMDH))\nwrite.csv(low_sample,file=here::here(\"data/03_data_cleaning/low_sample.csv\"),row.names = FALSE)\n```\n:::\n\n\nThis is very very low sample size, it makes no sense to keep them. I will exclude them from all the further analysis.\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\ndataset_analysis<-dataset_analysis%>%anti_join(low_sample,by=\"Observation_ID\")\n```\n:::\n\n\n## Saving all output files\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n# # Complete dataset that can be used for analysis\n# write.csv(dataset_analysis,file=here::here(\"data/04_data_analysis/dataset_analysis.csv\"),row.names = FALSE)\n# \n# \n# # Dataset that can be used to estimate lnRR\n# \n# dataset_lnRR<-dataset_analysis%>%\n#   filter(!is.na(lnRR_sign) & !is.na(lnRR_variance))\n# \n# write.csv(dataset_lnRR,file=here::here(\"data/04_data_analysis/dataset_lnRR.csv\"),row.names = FALSE)\n# \n# # Dataset that can be used to estimate SMD(H)\n# \n# dataset_SMDH<-dataset_analysis%>%\n#   filter(!is.na(SMDH_sign) & !is.na(SMDH_variance))\n# \n# write.csv(dataset_SMDH,file=here::here(\"data/04_data_analysis/dataset_SMDH.csv\"),row.names = FALSE)\n```\n:::\n\n",
    "supporting": [
      "06_data_preparation_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}